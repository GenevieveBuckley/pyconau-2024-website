title: Donâ€™t let your data model `drift` away!
start: 2023-08-19 13:30:00+09:30
end: 2023-08-19 14:00:00+09:30
room: b
track:
type: talk
abstract: "<p>Experimenting, building a model, and putting it into production takes
  a long time. The time difference might range from months to years. The distribution
  of data may vary throughout this time gap, resulting in differences between the
  data used to train and create the model and the data that the model encounters in
  the production environment. </p>\n<p>The performance of models degrades over time
  as a result of this drift, resulting in weak and declining predictive performance
  in predictive models. This is a typical occurrence, but it is a significant problem
  in performance-critical Machine Learning systems. </p>\n<p>Today's data is changing
  and evolving at a breakneck speed. It's critical to keep up with shifting data if
  you require high-performance models. As a result, it's critical to spot the point
  in production where your data diverges from the one it was trained on.</p>\n<p>Learn
  how these drifts affect our machine learning models and how to track and assess
  them in Python so that the model remains relevant in production and makes fair and
  unbiased predictions over time.</p>"
description: "<p>Due to changes in the actual world, production data might diverge
  or drift from the baseline data over time. When creating predictive models, which
  is the process of learning a model from previous data and applying it to fresh data
  for which we have no prior knowledge, things change throughout time, and model performance
  deteriorates. The model quality metric is the final criterion. It might be anything
  as simple as accuracy, mean error rate, or a downstream business KPI like click-through
  rate. </p>\n<p>As a result, monitoring and detecting these distribution deviations
  from the training or historic time period is critical for monitoring the health
  of deployed models, ensuring that they remain relevant in production and provide
  fair and unbiased predictions over time; otherwise, if these drifts go undetected,
  predictions will be incorrect, and business decisions may have a negative impact.</p>\n
  <p>Model drift may be caused by a variety of variables, the most common of which
  are data drift, prior probability drift and the concept drift. Because these drifts
  entail a statistical change in the data, a variety of statistical and model-based
  features, such as Kullback-Leibler divergence, Kolmogorov-Smirnov test, and others,
  might be used to detect them.</p>\n<p>The talk is an attempt to better understand
  the topic of model drifts and how they may be monitored and evaluated in real time
  using a repeatable method in order to minimize future mishappenings.</p>"
code: H3UBSS
speakers:
- CHHWUG
cw:
youtube_slug:
